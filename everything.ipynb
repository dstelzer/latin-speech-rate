{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook should run through the entire process of the experiment:\n",
    "# preprocessing the data, processing it, analyzing it, and turning it into graphs\n",
    "\n",
    "# For debugging; remove when no longer needed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Any global imports we need\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import bz2\n",
    "\n",
    "from tqdm.notebook import tqdm # Shiny progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, let's do the German/English experiment\n",
    "import celex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set up the data file (data/process.py)\n",
    "import data.process\n",
    "from csv import QUOTE_NONE\n",
    "# The first step is to convert the CELEX data to a format we can work with\n",
    "# This expects it to be in CSV format to start with (like the data provided by Dr Oh)\n",
    "# If you have raw HTML data exported from WebCelex, see data/preprocess.sh\n",
    "raw_file = 'data/deu_modified.csv' # The data provided by Oh\n",
    "data_file = 'data/german_TMP.pickle.bz2' # What we'll be exporting to\n",
    "parameters = {'delimiter':'\\t', 'escapechar':'\\x1b', 'strict':True, 'quoting':QUOTE_NONE} # The parameters of Oh's data\n",
    "# (See data/process.py for what delimiters should be used for raw CELEX data)\n",
    "counter = 'Word Mann'\n",
    "data.process.do_processing(parameters, raw_file, data_file, counter)\n",
    "print('Data saved to', data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635e3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the actual analysis (celex.py, analysis.py)\n",
    "parameters = {'stress':True, 'freq':'Word Mann', 'phon':'DISC', 'divider':' '}\n",
    "# For English, do {'stress':True, 'freq':'CobW', 'phon':'DISC'}\n",
    "# See celex.py for explanation\n",
    "analysis = celex.CelexAnalysis(**parameters, log=False)\n",
    "analysis.load_corpus(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca78fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have an analysis of German that we can use in various ways!\n",
    "# Here are the most important ones\n",
    "e1, e2 = analysis.do_things()\n",
    "print('Shannon entropy:', e1, 'Conditional entropy:', e2)\n",
    "analysis.corpus = Counter(dict(Counter(analysis.corpus).most_common(20000))) # Cut down to 20k most common\n",
    "analysis.count_unigrams()\n",
    "print('Syllable types in 20k most common:', len(analysis.unigrams))\n",
    "\n",
    "# Generate a new analysis object\n",
    "analysis = celex.CelexAnalysis(**parameters, log=False)\n",
    "analysis.load_corpus(data_file)\n",
    "# Calculate the e2 value (conditional entropy) at different sizes of corpus\n",
    "# On a log scale with 200 points, repeated 5 times\n",
    "analysis.calculate_reduced_e2(logscale=True, npts=200, n=5, bootstrap=False,\n",
    "                                save='math/german_log_TMP.pickle.bz2')\n",
    "print('German base finished')\n",
    "\n",
    "# And one more new analysis object\n",
    "analysis = celex.CelexAnalysis(**parameters, log=False)\n",
    "analysis.load_corpus(data_file)\n",
    "# Same as above but only up to a limit of 2m\n",
    "analysis.calculate_reduced_e2(logscale=True, npts=200, n=5, top=2_000_000, bootstrap=False, cut_top=True,\n",
    "                                save='math/german_log_cut2_TMP.pickle.bz2')\n",
    "print('German cut2 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's set up English in the same way\n",
    "# Once again, start with CSV; if you don't have CSV, use data/preprocess.sh\n",
    "raw_file = 'data/eng.csv' # Data scraped from WebCelex\n",
    "data_file = 'data/english_TMP.pickle.bz2' # What we'll be exporting to\n",
    "parameters = {'delimiter':'\\\\', 'escapechar':'\\x1b', 'strict':True, 'quoting':QUOTE_NONE} # Different delimiter here\n",
    "counter = 'CobW'\n",
    "data.process.do_processing(parameters, raw_file, data_file, counter)\n",
    "print('Data saved to', data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99faa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And analyze it\n",
    "parameters = {'stress':True, 'freq':'CobW', 'phon':'DISC'}\n",
    "analysis = celex.CelexAnalysis(**parameters, log=False)\n",
    "analysis.load_corpus(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe880f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, save some results for English\n",
    "# Now we have an analysis of German that we can use in various ways!\n",
    "# Here are the most important ones\n",
    "e1, e2 = analysis.do_things()\n",
    "print('Shannon entropy:', e1, 'Conditional entropy:', e2)\n",
    "analysis.corpus = Counter(dict(Counter(analysis.corpus).most_common(20000))) # Cut down to 20k most common\n",
    "analysis.count_unigrams()\n",
    "print('Syllable types in 20k most common:', len(analysis.unigrams))\n",
    "\n",
    "# Generate a new analysis object\n",
    "analysis = celex.CelexAnalysis(**parameters, log=False)\n",
    "analysis.load_corpus(data_file)\n",
    "# Calculate the e2 value (conditional entropy) at different sizes of corpus\n",
    "# On a log scale with 200 points, repeated 5 times\n",
    "analysis.calculate_reduced_e2(logscale=True, npts=200, n=5, bootstrap=False,\n",
    "                                save='math/english_log_TMP.pickle.bz2')\n",
    "print('English base finished')\n",
    "\n",
    "# Another more new analysis object\n",
    "analysis = celex.CelexAnalysis(**parameters, log=False)\n",
    "analysis.load_corpus(data_file)\n",
    "# Same as above but only up to a limit of 2m\n",
    "analysis.calculate_reduced_e2(logscale=True, npts=200, n=5, top=2_000_000, bootstrap=False, cut_top=True,\n",
    "                                save='math/english_log_cut2_TMP.pickle.bz2')\n",
    "print('English cut2 finished')\n",
    "\n",
    "# Then for English, we do one test we didn't do for German: bootstrapping to higher size\n",
    "analysis = celex.CelexAnalysis(**parameters, log=False)\n",
    "analysis.load_corpus(data_file)\n",
    "top = analysis.tokens * 10 # Go one order of magnitude higher than we have\n",
    "analysis.calculate_reduced_e2(n=2, logscale=True, npts=200, bootstrap=True, top=top,\n",
    "                              save='math/english_bootstrap_TMP.pickle.bz2')\n",
    "print('English bootstrap finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af9b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's time for the most important part: the Latin analysis!\n",
    "# This assumes you have the PHI plaintext data stored in the appropriate place\n",
    "# For me, that's ~/cltk_data/latin/text/phi5\n",
    "# I'm not sure if I can legally distribute it, but you should be able to find it online without too much difficulty\n",
    "\n",
    "# You also need my custom version of cltk installed\n",
    "# Put the path to that here\n",
    "import sys\n",
    "sys.path.insert(1, '../CLTK/cltk/')\n",
    "import cltk\n",
    "\n",
    "# WARNING: For some reason, this sometimes (inconsistently!) throws errors with recent versions of NLTK.\n",
    "# If you get errors, roll back to NLTK version 3.5 (pip3 install nltk==3.5) and that should fix it.\n",
    "# I suspect this has to do with breaking changes made to NLTK, which CLTK was updated to support...\n",
    "# ...but my fork doesn't have those fixes, and I'm hesitant to mess with it much at this point.\n",
    "# So rolling back NLTK is the safer choice.\n",
    "\n",
    "import data.latin.corpus as cps\n",
    "JUSTINIAN = 'LAT2806' # Justinian's author code\n",
    "\n",
    "solo = Path('./data/latin/auth_solo_TMP') # The directory where authors' solo corpora are stored\n",
    "solo.mkdir(exist_ok=True) # Create it if it doesn't already exist\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed45312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first generate an overview of authors' token counts, to ensure the corpus is working\n",
    "# If this doesn't work, your PHI5 or CLTK installation is broken\n",
    "# (This is designed to test the macronizer and syllabifier as well, so it takes a while)\n",
    "data = cps.PHI5Corpus().get_author_data()\n",
    "with bz2.open('authors_TMP.pickle.bz2', 'w') as f:\n",
    "    pickle.dump(data, f)\n",
    "with open('authors_TMP.tsv', 'w') as f:\n",
    "    for (name, id), words in sorted(data.items()):\n",
    "        f.write(f'{name}\\t{id}\\t{words}\\n')\n",
    "for (name, id), words in sorted(data.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    print(name, id, words) # This will show you if it's working or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e042158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate two versions, one with Justinian (_complete) and one without\n",
    "# But since we're rewriting the code for this notebook, why not do it in a more efficient way?\n",
    "# Let's only generate the smaller one, and then add Justinian to it\n",
    "# (Why generate this at all, instead of just going over each author once, and then stitching them together?)\n",
    "# (Because this gives us some extra redundancy against errors, which was very important early on in the project!)\n",
    "CORPUS_SMALL = './data/latin/phi5_TMP.pickle.bz2'\n",
    "cps.PHI5Corpus().process_and_save(CORPUS_SMALL, authorial=True, shuffle=True, exclude=(JUSTINIAN,), overwrite=False)\n",
    "# Make overwrite=True to replace a previous run if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_BIG = './data/latin/phi5_complete_TMP.pickle.bz2'\n",
    "tmp = solo/(JUSTINIAN+'.pickle.bz2') # We're doing this before we deal with the rest of the solo folder, but why duplicate effort?\n",
    "cps.PHI5Corpus().process_and_save(tmp, authorial=True, shuffle=True, include=(JUSTINIAN,), overwrite=False)\n",
    "with bz2.open(CORPUS_SMALL, 'r') as f:\n",
    "    small = Counter(pickle.load(f))\n",
    "with bz2.open(tmp, 'r') as f:\n",
    "    just = Counter(pickle.load(f))\n",
    "new = small + just\n",
    "with bz2.open(CORPUS_BIG, 'w') as f:\n",
    "    pickle.dump(dict(new), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we generate corpora for each \"important\" author individually, plus a MISC one for all the others\n",
    "# Justinian is specifically excluded from all of these!\n",
    "c = cps.PHI5Corpus()\n",
    "IMPORTANT_PLUS_JUSTINIAN = cps.IMPORTANT_AUTHORS | {JUSTINIAN} # Exclude Justinian from the MISC category\n",
    "c.process_and_save(solo/'MISC.pickle.bz2', exclude=IMPORTANT_PLUS_JUSTINIAN,\n",
    "                   overwrite=False, shuffle=True, authorial=True) # Change overwrite to true to replace an existing file\n",
    "for auth in tqdm(cps.IMPORTANT_AUTHORS):\n",
    "    c.process_and_save(solo/f'{auth}.pickle.bz2', authorial=True, include=(auth,), overwrite=False, shuffle=True)\n",
    "    # Again, change overwrite to true if you want to replace an existing file\n",
    "    # It's left false by default so that, if this cell gets interrupted, you can skip the ones that were already finished\n",
    "# Justinian was handled in a previous cell, but if you need to:\n",
    "# cps.process_and_save(solo/(JUSTINIAN+'.pickle.bz2'), authorial=True, include=(JUSTINIAN,), overwrite=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4cdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And subtract each of them from the complete collection\n",
    "def subtract_authors(complete=False):\n",
    "    result = Path('./data/latin/') / ('auth_complete_TMP' if complete else 'auth_TMP')\n",
    "    result.mkdir(exist_ok=True)\n",
    "    #c = cps.PHI5Corpus() # We don't actually need a corpus object here, we're just working with Counters\n",
    "    with bz2.open(CORPUS_BIG if complete else CORPUS_SMALL, 'r') as f: # Choose the appropriate full corpus\n",
    "        full = Counter(pickle.load(f))\n",
    "    for auth in tqdm(list(solo.glob('*.pickle.bz2'))):\n",
    "        if auth.name.startswith(JUSTINIAN) and not complete:\n",
    "            print('(Skipping Justinian)')\n",
    "            continue\n",
    "        with bz2.open(auth, 'r') as f:\n",
    "            d = Counter(pickle.load(f))\n",
    "        new = full - d\n",
    "        if sum(new.values()) != sum(full.values()) - sum(d.values()): # Sanity check\n",
    "            raise ValueError(auth, sum(new.values()), sum(full.values()), sum(d.values()))\n",
    "        with bz2.open(result / auth.name, 'w') as f:\n",
    "            pickle.dump(dict(new), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58154019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with Justinian\n",
    "subtract_authors(complete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f624e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it without Justinian\n",
    "subtract_authors(complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f304be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now that we have all this data, we can also calculate accurate type and token counts for each author\n",
    "with bz2.open(CORPUS_BIG, 'r') as f:\n",
    "    d_tot = pickle.load(f)\n",
    "ctype, ctoken = len(d_tot), sum(d_tot.values())\n",
    "print('Total (big)', ctype, ctoken)\n",
    "with bz2.open(CORPUS_SMALL, 'r') as f:\n",
    "    d_tot = pickle.load(f)\n",
    "ctype, ctoken = len(d_tot), sum(d_tot.values())\n",
    "print('Total (small)', ctype, ctoken)\n",
    "for auth in solo.glob('*.pickle.bz2'):\n",
    "    with bz2.open(auth, 'r') as f:\n",
    "        d = pickle.load(f)\n",
    "    ttype, ttoken = len(d), sum(d.values())\n",
    "    print(auth.stem, ttype, ttoken)\n",
    "    if auth.name.startswith(JUSTINIAN):\n",
    "        print('\\t(That\\'s Justinian; skipped)')\n",
    "    else:\n",
    "        ctype -= ttype; ctoken -= ttoken\n",
    "print('Unaccounted for', ctype if ctype > 0 else 0, ctoken) # We expect ctype to be negative because many authors use the same types\n",
    "# But ctoken should definitely be zero, and if it's not, then something's gone wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our various Latin corpora produced, we can analyze them the same way we did English and German\n",
    "# See analyze.py for details on this\n",
    "import analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c259951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate raw entropy, first of all\n",
    "analysis = analyze.Analysis()\n",
    "analysis.load_corpus(CORPUS_SMALL)\n",
    "e1, e2 = analysis.do_things()\n",
    "print('Shannon entropy', e1, 'Conditional entropy', e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b830a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now cut it down randomly for extrapolation purposes\n",
    "# This takes less time than for English or German because the corpus is smaller\n",
    "analysis = analyze.Analysis(log=False)\n",
    "analysis.load_corpus(CORPUS_SMALL)\n",
    "analysis.calculate_reduced_e2(logscale=True, npts=200, n=5, bootstrap=False,\n",
    "                                save='math/latin_log_TMP.pickle.bz2')\n",
    "print('Done') # Have to do something else afterward to avoid Jupyter printing all the raw output from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And do the same for the complete corpus, including Justinian (in order to compare the two)\n",
    "analysis = analyze.Analysis(log=False)\n",
    "analysis.load_corpus(CORPUS_BIG)\n",
    "analysis.calculate_reduced_e2(logscale=True, npts=200, n=5, bootstrap=False,\n",
    "                                save='math/latin_log_complete_TMP.pickle.bz2')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Path('./math/latin_auth_TMP')\n",
    "out.mkdir(exist_ok=True)\n",
    "inp = Path('./data/latin/auth_TMP')\n",
    "for auth in tqdm(list(inp.glob('*.pickle.bz2'))):\n",
    "    if auth.name.startswith(JUSTINIAN) or auth.name.startswith('MISC'):\n",
    "        print('Skipping', auth.name)\n",
    "        continue\n",
    "    path = out / auth.name\n",
    "    if path.exists() and True: # Change True to False to overwrite existing ones\n",
    "        print(auth.name, 'already exists')\n",
    "        continue\n",
    "    analysis = analyze.Analysis(log=False)\n",
    "    analysis.load_corpus(auth)\n",
    "    analysis.calculate_reduced_e2(logscale=True, npts=200, n=1, bootstrap=False, # Only n=1 for these\n",
    "                                  save=path)\n",
    "# Strictly speaking we should then do the same again, with the _complete corpus\n",
    "# But for the sake of time I'm not doing that here; it works exactly the same as the above\n",
    "# Just change out and inp to latin_auth_complete_TMP and latin/auth_complete_TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've got all the data files we need, stored safely in ./math\n",
    "# The next step is to plot 'em!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "\n",
    "import plots\n",
    "# See plots.py for the exact details of how the figures were generated\n",
    "# Warning: depending on your version of Numpy, the curve-fitting may produce spurious warnings!\n",
    "# Numpy warns whenever you take a fractional power of a negative number, even if the result is real\n",
    "# Everything works fine, though, and you can safely ignore these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff460435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve-fitting for German\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "d = plots.Dataset('math/german_log_TMP.pickle.bz2')\n",
    "d.fit_curve()\n",
    "d.mark_curve(xmax = max(d.xs)*10)\n",
    "d.draw_data('b.')\n",
    "plt.plot(d.xs, d.ys, '.', color='#70c070', label='Data') # Green for German\n",
    "d.draw_curve('-k')\n",
    "plt.plot(d.pxs, d.pys, '-k', alpha=0)\n",
    "d.draw_asymptote('--', 'g', False)\n",
    "plt.axhline(y=d.popt[0], linestyle='--', color='r', alpha=0)\n",
    "plt.xlabel('Corpus Size (tokens)')\n",
    "plt.ylabel('Information Density (bits/syl)')\n",
    "print(d.popt)\n",
    "print(max(d.ys))\n",
    "\n",
    "newticks = [6.4]\n",
    "plt.yticks(list(plt.yticks()[0]) + newticks) # Add an extra tick to the y-axis\n",
    "plt.legend()\n",
    "\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolating English and German\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "d2 = plots.Dataset('math/english_log_cut2_TMP.pickle.bz2')\n",
    "d2.fit_curve()\n",
    "d2.mark_curve(xmax = max(d2.xs)*10)\n",
    "plt.plot(d2.xs, d2.ys, '.', color='#7070ff', label='English')\n",
    "d2.draw_asymptote('-', 'b', False, label=None)\n",
    "d2.draw_curve('-k', label=None)\n",
    "print(d2.popt)\n",
    "print(max(d2.ys))\n",
    "d2.draw_asymptote('--', 'r', False, override=6.98057, label=None)\n",
    "\n",
    "d3 = plots.Dataset('math/german_log_cut2_TMP.pickle.bz2')\n",
    "d3.fit_curve()\n",
    "d3.mark_curve(xmax = max(d3.xs)*10)\n",
    "plt.plot(d3.xs, d3.ys, '.', color='#70c070', label='German')\n",
    "d3.draw_asymptote('-', 'g', False, label=None)\n",
    "d3.draw_curve('-k')\n",
    "print(d3.popt)\n",
    "print(max(d3.ys))\n",
    "d3.draw_asymptote('--', 'r', False, override=6.082567690505002, label=\"Previous Result\")\n",
    "plt.legend()\n",
    "plt.xlabel('Corpus Size (tokens)')\n",
    "plt.ylabel('Information Density (bits/syl)')\n",
    "\n",
    "d3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d78eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latin!\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "d = plots.Dataset('math/latin_log_TMP.pickle.bz2')\n",
    "d.fit_curve()\n",
    "d.mark_curve(xmax = max(d.xs)*10) # *10 for Latin, *100 if you're doing German\n",
    "plt.plot(d.xs, d.ys, '.', color='#70c4ff', label='Data')\n",
    "d.draw_curve('-k')\n",
    "d.draw_asymptote('--', 'b', False)\n",
    "plt.xlabel('Corpus Size (tokens)')\n",
    "plt.ylabel('Information Density (bits/syl)')\n",
    "print(d.popt)\n",
    "print(max(d.ys))\n",
    "plt.legend()\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8acc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of authors\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "vals = []\n",
    "\n",
    "d0 = plots.Dataset('math/latin_log_TMP.pickle.bz2') # Use _complete to include the Digesta\n",
    "d0.fit_curve()\n",
    "\n",
    "for i, auth in enumerate(Path('math/latin_auth_TMP').glob('*.pickle.bz2')): # Use _complete to include the Digesta\n",
    "\tif '0474' in auth.stem and False: continue # Remove Cicero here if you want\n",
    "\td = plots.Dataset(auth)\n",
    "\td.fit_curve()\n",
    "\td.mark_curve(xmax=max(d0.xs)*10, npts=500)\n",
    "\t# old: random.choice('oDv^s')\n",
    "\td.draw_data('.'+'bgmycbgmycbgcmyb'[i])\n",
    "\td.draw_curve('k')\n",
    "\tvals.append(d.popt[0])\n",
    "\tprint(f'Without {auth.stem.split(\".\")[0]}: {d.popt[0]}')\n",
    "\n",
    "d0.draw_asymptote('-', 'r', False)\n",
    "\n",
    "plt.xlabel('Corpus Size (tokens)')\n",
    "plt.ylabel('Information Density (bits/syl)')\n",
    "\n",
    "print(f'Actual value: {d0.popt[0]}')\n",
    "print(f'Approximated value: {sum(vals)/len(vals)}')\n",
    "print(f'Approximated standard error: {np.std(vals, ddof=1)}')\n",
    "\n",
    "d0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a92f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "d = plots.Dataset('math/latin_log_complete_TMP.pickle.bz2')\n",
    "d.fit_curve()\n",
    "d.mark_curve(xmax = max(d.xs)*10)\n",
    "plt.plot(d.xs, d.ys, '.', color='#ff7070', label='With Digesta')\n",
    "print(d.popt)\n",
    "\n",
    "d2 = plots.Dataset('math/latin_log_TMP.pickle.bz2')\n",
    "d2.fit_curve()\n",
    "d2.mark_curve(xmax = max(d2.xs)*10)\n",
    "plt.plot(d2.xs, d2.ys, '.', color='#70c4ff', label='Without Digesta')\n",
    "print(d2.popt)\n",
    "\n",
    "d.draw_asymptote('--', 'r', False, label=None)\n",
    "d2.draw_asymptote('--', 'b', False, label=None)\n",
    "\n",
    "d.draw_curve('-k', label=None)\n",
    "d2.draw_curve('-k', label=None)\n",
    "\n",
    "plt.xlabel('Corpus Size (tokens)')\n",
    "plt.ylabel('Information Density (bits/syl)')\n",
    "plt.legend(loc='lower right')\n",
    "d2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26576f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
